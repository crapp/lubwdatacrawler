#     Data Crawler for LUBW Measurement Stations
#     Copyright (C) 2014  Christian Rapp
#
#     This program is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This program is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.

__author__ = 'Christian Rapp'

import re
import logging
import bs4
import time
import urllib.request
import urllib.parse


class MeasurementParser:
    """
    The MeasurementParser Class uses beautifulsoup4 to parse the websites of measurement stations
    of the Landesanstalt fuer Umwelt, Messungen und Naturschutz Baden-Wuerttemberg

    Attributes:
        args: List generated by argparse lib holding command line options
        urllist: List of URLs to parse
        crawl: Boolean inicating if we should continue the crawl cycle
        log: Logging object
        db: SQLiteConnector object
    """

    def __init__(self, args, db):
        """Inits an Object of MeasurementParser"""
        self.args = args
        self.urllist = []
        self.crawl = True
        self.log = logging.getLogger("my_logger")
        self.db = db

        self._parseurls()

    def _parseurls(self):
        """
        Private method that checks if the urls provided are valid and stores them in self.urllist

        Raises:
            Exception: If list or url is not valid. List must be comma separated and url of type
                       http(s)://somewhere_in_the_web.org/foo.htm(l)
        """
        urls = str.split(self.args.url, ',')
        if len(urls) > 0:
            validurl = re.compile(r"^https?://.+\.html?$")
            for url in urls:
                if not re.match(validurl, url):
                    raise Exception(url + " is not a valid URL. e.g. https://myUrl.html")
                self.log.info("Found URL: " + url)
                self.urllist.append(url)
        else:
            raise Exception("URLs must be a comma separated list")

    def crawlwebsites(self):
        """
        Crawls all specified websites and extracts the measurement data of the stations from a table
        via beautifulsoup4

        If you pass a parsetime as command line argument (integer as seconds), the method will run
        its own loop and wait for the specified seconds until next parse cycle.
        """
        while self.crawl:
            self.log.info("Starting next crawl cycle...")
            starttime = time.clock()
            for url in self.urllist:
                try:
                    response = urllib.request.urlopen(url)
                    self.log.debug(url + " code: " + str(response.getcode()))
                    html = response.read()
                    #TODO: Check response code with get_code()
                    soup = bs4.BeautifulSoup(html, "html.parser")
                    print(soup)
                    #TODO: Station Name could have special characters like german umlauts. Can
                    #      sqlite safely operate with them?
                    #TODO: Maybe better to find div with id DatenContainer and use the html returned
                    #      by this find to do the rest of the searches. Should be faster
                    station_name = soup.find("div", id="Name")
                    self.log.info("Station Name: " + station_name.get_text())
                    measurementdatetime = soup.find("div", id="Datum")
                    valuestable = soup.find("table", id="WerteTabelle")
                    print(valuestable.get_text())
                    # find all components the station is measuring. This components define the table
                    # columns in the database. additionally the parent object is the table row. we
                    # traverse all siblings in this row later to get to the current reading.
                    components = valuestable.find_all("td", class_="cell3d-ueb02")
                    self.log.debug("Found " + str(len(components)) + " components in table")
                    comp_values_map = {}
                    for c in components:
                        #get all td elements
                        rowdata = c.parent.find_all("td", class_="cell3d-dat")
                        self.log.debug("Found " + str(len(rowdata)) + " data cells in table row")
                        datalist = []
                        i = 0
                        #loop over the list of td elements and get the text
                        for td in rowdata:
                            if i == 0:
                                i += 1
                                continue
                            if i > 4:
                                break
                            datalist.append(td.get_text())
                            i += 1
                        self.log.debug("Component " + c.get_text(strip=True) + " has " +
                                       ','.join(datalist) + " values")
                        #append the measurement date time
                        datalist.append(measurementdatetime.get_text(strip=True))
                        comp_values_map[c.get_text(strip=True)] = datalist
                    #print(components)
                    #all data is collected now tell the database to do its job
                    self.db.insert(station_name.get_text(strip=True), comp_values_map)
                except urllib.error.URLError as ex:
                    self.log.error("Can not parse URL: " + url + "\n" + str(ex))
                except AttributeError as ex:
                    self.log.error("HTML Parser error for URL: " + url + "\n" + str(ex))
                    self.log.error("No data will be inserted into Database")

            endttime = time.clock()
            self.log.info("Crawl cycle finished in " + str(endttime-starttime) + " seconds")
            #if parsetime is set we sleep here and crawl the data once again
            if self.args.parsetime:
                self.log.debug("Sleeping for " + str(self.args.parsetime) + " seconds")
                time.sleep(self.args.parsetime)
            else:
                #exit the program...
                self.crawl = False
